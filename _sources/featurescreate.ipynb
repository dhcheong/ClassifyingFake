{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b81a92-5c02-429f-92f8-2ec347bed5a3",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba853f7e-a962-41ce-a2d7-f3f2b9473e13",
   "metadata": {},
   "source": [
    "This section will focus on creating features from cleaned-text data by counting word frequency and some limitations regarding using such a method. Also, we will talk about how we should handle non-text columns in the data, especially the columns with categorical variables. Additionally, we will take closer look into our training dataset to see if we can create more helpful features other than given columns.\n",
    "\n",
    "First, we will load the data we cleaned on the previous page, and we will split this into train and test datasets. We split the dataset to ensure the test dataset does not influence when creating a pipeline. Technically, we are not supposed to know our test set before the completion of our pipeline. **After we create the pipeline, we will apply the same pipeline we used for the train set to the test set to ensure both datasets have the same dimension.**  Please refer to [here](Plan.ipynb) if you want to know how I splited the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c14ea1c-d5a7-4984-a726-7480ffaf8456",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3a3ffe8-93cb-41fc-ab23-119b3228e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/cleaned_fake_job_postings.csv\")\n",
    "y = data.iloc[:, -1]\n",
    "X = data.iloc[:, :-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1913d-3e73-45d6-b1d5-a9a878548acf",
   "metadata": {},
   "source": [
    "Before we move on to the next step, we will create new dataframes for the train and test set and save these as csv to data folder. We will use the following code to do the work.\n",
    "\n",
    "```python \n",
    "# Resetting Index before we create new csv file for train and test\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "y_test.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Combining X and y (which is just fraudulent column) \n",
    "X_train[\"fraudulent\"] = y_train\n",
    "X_test[\"fraudulent\"] = y_test\n",
    "\n",
    "# Save the combined dataframe to csv\n",
    "X_train.to_csv(\"./data/train_set.csv\")\n",
    "X_test.to_csv(\"./data/test_set.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
