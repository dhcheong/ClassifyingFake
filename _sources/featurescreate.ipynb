{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b81a92-5c02-429f-92f8-2ec347bed5a3",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba853f7e-a962-41ce-a2d7-f3f2b9473e13",
   "metadata": {},
   "source": [
    "This page will focus on creating features from cleaned-text data by counting word frequency and some limitations regarding using such a method. Also, we will talk about how we should handle non-text columns in the data, especially the columns with categorical variables. \n",
    "\n",
    "First, we will load the data we cleaned on the previous page, and we will split this into train and test datasets. We split the dataset to ensure the test dataset does not influence when creating a pipeline. Technically, we are not supposed to know our test set before the completion of our pipeline. **After we create the pipeline, we will apply the same pipeline we used for the train set to the test set to ensure both datasets have the same dimension.**  Please refer to [here](Plan.ipynb) if you want to know how I splited the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c14ea1c-d5a7-4984-a726-7480ffaf8456",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3a3ffe8-93cb-41fc-ab23-119b3228e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/cleaned_fake_job_postings.csv\")\n",
    "y = data.iloc[:, -1]\n",
    "X = data.iloc[:, :-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1913d-3e73-45d6-b1d5-a9a878548acf",
   "metadata": {},
   "source": [
    "Before we move on to the next step, we will create new dataframes for the train and test set and save these as csv to data folder. We will use the following code to do the work.\n",
    "\n",
    "```python \n",
    "# Resetting Index before we create new csv file for train and test\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "y_test.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Combining X and y (which is just fraudulent column) \n",
    "X_train[\"fraudulent\"] = y_train\n",
    "X_test[\"fraudulent\"] = y_test\n",
    "\n",
    "# Save the combined dataframe to csv\n",
    "X_train.to_csv(\"./data/train_set.csv\")\n",
    "X_test.to_csv(\"./data/test_set.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959838c7-86ff-44e5-98c2-608ec0e95b94",
   "metadata": {},
   "source": [
    "As we mentioned earlier, we want each word to be a feature of the dataset. To do this, we will use the **Bag of Words** method that will ensure each description in the dataset is represented only with the word count (e.g., if the description contains the word, the column will mark it as 1, otherwise 0). However, only using the Bag of Words is somewhat problematic. Since frequency is dependent on the length of the text, longer texts can calculate a higher frequency for some unnecessary words that rarely appear across the data as a whole. This clearly can cause big trouble when we come to feature selection. \n",
    "\n",
    "To remedy this problem, we will standardize the word frequency using **TF-IDF (Term Frequency-Inverse Document Frequency)** so that all frequencies can be weighted. It is a numerical statistic intended to reflect how important a word is to a document in a collection, and here is how we calculate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86bf76-f3f3-40c0-9df0-8c4a853b628d",
   "metadata": {},
   "source": [
    "$$ \\text{TF-IDF} = \\text{TF} \\times \\text{IDF} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$TF = \\frac{\\text{number of times the word appears in the description}}{\\text{total number of the word in the entire dataset}}$$\n",
    "\n",
    "$$IDF = \\log ( \\frac{\\text{number of description in the dataset}}{\\text{number of description that contained the word}} )$$\n",
    "\n",
    "Term Frequency (TF) measures how frequently a term occurs in a document. Inverse Document Frequency (IDF) is a factor that diminishes the weight of terms that occur very frequently in the document and increases the weight of words that occur rarely. As you observe here, as the word appears less frequently throughout the dataset, the IDF increases which decreases TF-IDF as a result. We gives more weight on the words that appear frequently across the entire dataset. This way, we can avoid possible outlier/confounding features in our dataset.\n",
    "\n",
    "```{note}\n",
    "In the original project, we had to extracting the features and converting those using TF-IDF were a seperate step. In Python, sklearn has very useful feature extraction encoder, \"TfidfVectorizer\", and this combines the extraction and conversion into a single step.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "228b3e12-98cc-45fd-9113-b060a43c05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Import train dataset\n",
    "train_data = pd.read_csv(\"./data/train_set.csv\")\n",
    "train_data[\"description\"].fillna('no_description', inplace = True)\n",
    "vec = TfidfVectorizer(smooth_idf=True)\n",
    "tfidf_description = vec.fit_transform(train_data[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa531e31-9c81-4df1-98d1-15742a351858",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "We have some NAs in the description column, which will cause the error with TfidfVectorizer if we don't impute it. I decided to replace all NAs with \"no_description\" because NA may indicate a strong signal for fraudulent. For instance, a fraudulent posting might have no job description.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "beafbdc4-42e5-4b00-9c81-a3763c3dc09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaab</th>\n",
       "      <th>aab</th>\n",
       "      <th>aabc</th>\n",
       "      <th>aabd</th>\n",
       "      <th>aabf</th>\n",
       "      <th>aac</th>\n",
       "      <th>aaccd</th>\n",
       "      <th>aachen</th>\n",
       "      <th>...</th>\n",
       "      <th>zumero</th>\n",
       "      <th>zur</th>\n",
       "      <th>zurb</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zusammenarbeitest</th>\n",
       "      <th>zusammenbringt</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zyfax</th>\n",
       "      <th>zyka</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa  aaa  aaab  aab  aabc  aabd  aabf  aac  aaccd  aachen  ...  \\\n",
       "0  0.164839  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "1  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "2  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "3  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "4  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "\n",
       "   zumero  zur  zurb  zurich  zusammenarbeitest  zusammenbringt  zweig  zyfax  \\\n",
       "0     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "1     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "2     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "3     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "4     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "\n",
       "   zyka  zynga  \n",
       "0   0.0    0.0  \n",
       "1   0.0    0.0  \n",
       "2   0.0    0.0  \n",
       "3   0.0    0.0  \n",
       "4   0.0    0.0  \n",
       "\n",
       "[5 rows x 40989 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name = vec.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_description.toarray(), columns = features_name)\n",
    "df_tfidfvect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c618c-cb50-4e7a-a2fa-c3338ceea110",
   "metadata": {},
   "source": [
    "We will save our TfidfVectorizer as a pickle so that we can use it later to the test dataset.\n",
    "\n",
    "```python\n",
    "with open('./pickle/tfidfvec.pkl', 'wb') as f:\n",
    "    pickle.dump(vec, f)\n",
    "```\n",
    "```{warning}\n",
    "This step is essential because if we don't use the same encoder to train and test the set, it might raise the dimension error in any machine learning algorithm. For example, if we use a different encoder, the feature \"planner\" in the training set will not appear in the test set so that we can get the numbers of a feature. To avoid this error, we should apply the encoder that was trained by the training set to the test set. Sometimes, people combine train and test sets and run TfidfVectorizer to avoid this error, but we must always assume that we don't have any test set yet.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
