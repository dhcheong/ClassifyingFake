{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008af257-67ba-4453-b38e-2d16e17e20be",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2b0e1-134a-4935-8fc5-d9676f61b77a",
   "metadata": {},
   "source": [
    "Feature creation is one of the most critical parts of this project since the data largely depends on text data which requires delicate text mining and data cleaning. The most common way of creating features using text data is to use a word frequency, and accurate recording frequency necessitates careful data cleaning. On this page, we will only perform data cleaning on the \"description\" columns to create features based on this column. Note that we only want to use the text data in the `description` column, not the `company profile`, `requirement` and `benefit` columns, since nearly every job posting has `description` while `company profile`,  `requirement` and `benefit` columns are often missing. Please refer to [here](DataDes.ipynb) if you want to see how many variables are missing for each column.\n",
    "\n",
    "```{warning}\n",
    "The other project combined 'description' with 'company profile,' 'requirement', and 'benefit' into a single column and performed the analysis, but I believe this can cause to confound our result. Even though all these columns consist of text data, each column must be treated independently and should not be combined without any special reasons.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "efc63b87-f9fb-4aa4-9dfc-cc4ca7fc6155",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b3f9495f-e389-4613-93d8-e2a5979461d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/fake_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca173a7-23b3-41c5-8661-3e494507b9b2",
   "metadata": {},
   "source": [
    "### 1. Decapitalization \n",
    "\n",
    "This steps must be done before removing stopwords. Otherwise, Python will treat \"And\" and \"and\" differently. Decapitalization of a text data can make our frequency more accurate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8e9334dc-d5c9-48a3-aeae-69ccbab791b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"] = data[\"description\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b06ae9-0a4f-474b-9fb9-f628e2e0a5bf",
   "metadata": {},
   "source": [
    "### 2. Remove Insignificant Words and Punctuation\n",
    "\n",
    "The stopwords and punctuation must be removed since their frequency would not significantly improve prediction. We also pulled any strings that related to HTML/URL and emojis. **Note that we substitute those removed URLs and HTML with \"url\" and \"html\"** since the presence of URL/HTML can be significant in predicting, so we still want to count those. (For example, many spam emails include more URL/HTML than ham emails.) After we remove these words, we can observe that the words such as \"\\xa\" and \"amp\" still exist in a sentence, so we will manually remove those words as much as we can. We will also remove all number from the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "08af6e10-804c-4a0a-acdf-064302401991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step loads stopwords from nltk library \n",
    "stop = set(stopwords.words('english'))\n",
    "stop = list(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b0d3fd6c-af71-41d9-9cdb-6f08a6e78be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'#url_\\w*#')\n",
    "    return url.sub(r'url ',str(text))\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', str(text))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'html ',str(text))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return ' '.join(word for word in str(words).split() if word not in stop)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    return ' '.join(word.strip(string.punctuation) for word in str(words).split())\n",
    "\n",
    "def remove_dirty_words(words):\n",
    "    dirty_words=re.compile(r'[^\\x00-\\x7F]+|(&amp)|\\d|[^\\w\\s]')\n",
    "    return dirty_words.sub(r' ',str(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "04d59815-6221-4f44-abbc-d131ae2b8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"] = data.description.apply(remove_URL)\n",
    "data[\"description\"] = data.description.apply(remove_html)\n",
    "data[\"description\"] = data.description.apply(remove_emoji)\n",
    "data[\"description\"] = data.description.apply(remove_dirty_words)\n",
    "data[\"description\"] = data.description.apply(remove_punctuation)\n",
    "data[\"description\"] = data.description.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d76d46-41a5-467b-b686-64fd88f7ebdf",
   "metadata": {},
   "source": [
    "### 3. Lemmatization vs. Stemming\n",
    "\n",
    "The primary goal of data cleaning in this project is to get the correct frequency of the words. Therefore we need lemmatization or stemming so that the words \"love\" and \"loving\" count as the same. In the original project, we used lemmatization instead of stemming, but this time, we will focus on using stemming for these reasons:\n",
    "\n",
    "1. **The stemming algorithm is more efficient and faster than lemmatization.** Both methods return the root word, but in lemmatization, we use WordNet corpus and a corpus for stop words to produce a lemma which makes it slower than stemming.\n",
    "\n",
    "2. **Stemming is enough for this project.** The main difference between these two methods is that lemmatization returns the actual word, while stemming doesn't have to. Since we don't need the actual word, stemming provides a more straightforward solution. \n",
    "\n",
    "```{note}\n",
    "The original project used lemmatization for no reasons.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49f6d420-fced-421a-94bb-3215a5f7704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f0c7fac6-cd87-4920-996c-7a0c03e70405",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description\"] = data.description.apply(stemSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c281a-056d-438d-b32b-2dc79f8174ad",
   "metadata": {},
   "source": [
    "We will store this dataset as CSV so that we can use it in the next step!\n",
    "\n",
    "```python \n",
    "data.to_csv(\"./data/cleaned_fake_job_postings.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
