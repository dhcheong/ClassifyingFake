{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88c0707-83a9-4371-b56b-fad3672f6dc5",
   "metadata": {},
   "source": [
    "# Creating Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebe569-237c-43d3-889d-5d9bf062ddae",
   "metadata": {},
   "source": [
    "In this section, we will combine everything we discussed in \"Feature Creation\" and \"Feature Selection\", and make a complete pipeline for preprocessing. This will be extremely useful when it comes to processing test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14aa531b-28fb-4ba7-bf93-9bf339322c10",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a080e29b-e993-48f1-9887-fb8f785631b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(df, condition = \"train\"):\n",
    "    \"\"\"Extract text features from columns\"\"\"\n",
    "    \"\"\"Input: Data frame \"\"\"\n",
    "    \"\"\"Output: Data frame of extracted text features with their TF-IDF\"\"\"\n",
    "    \n",
    "    df[\"description\"] = df[\"description\"].str.lower()\n",
    "    df[\"requirements\"] = df[\"requirements\"].str.lower()\n",
    "    df[\"benefits\"] = df[\"benefits\"].str.lower()\n",
    "    df[\"title\"] = df[\"title\"].str.lower()\n",
    "    \n",
    "    # Remove all NA \n",
    "    df[\"description\"].fillna(\" \", inplace = True)\n",
    "    df[\"requirements\"].fillna(\" \", inplace = True)\n",
    "    df[\"benefits\"].fillna(\" \", inplace = True)\n",
    "    df[\"title\"].fillna(\" \", inplace = True)\n",
    "    \n",
    "    # Remove unnecessary words and punctuation / stemming\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop = list(stop)\n",
    "\n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'#url_\\w*#')\n",
    "        return url.sub(r'url ',str(text))\n",
    "\n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r' ', str(text))\n",
    "\n",
    "    def remove_html(text):\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'html ',str(text))\n",
    "\n",
    "    def remove_stopwords(words):\n",
    "        return ' '.join(word for word in str(words).split() if word not in stop)\n",
    "\n",
    "    def remove_punctuation(words):\n",
    "        return ' '.join(word.strip(string.punctuation) for word in str(words).split())\n",
    "\n",
    "    def remove_dirty_words(words):\n",
    "        dirty_words=re.compile(r'[^\\x00-\\x7F]+|(&amp)|\\d|[^\\w\\s]')\n",
    "        return dirty_words.sub(r' ',str(words))\n",
    "    \n",
    "    def stemSentence(sentence):\n",
    "        porter=PorterStemmer()\n",
    "        token_words=word_tokenize(sentence)\n",
    "        token_words\n",
    "        stem_sentence=[]\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(porter.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "    \n",
    "    df[\"description\"] = df.description.apply(remove_URL)\n",
    "    df[\"description\"] = df.description.apply(remove_html)\n",
    "    df[\"description\"] = df.description.apply(remove_emoji)\n",
    "    df[\"description\"] = df.description.apply(remove_dirty_words)\n",
    "    df[\"description\"] = df.description.apply(remove_punctuation)\n",
    "    df[\"description\"] = df.description.apply(remove_stopwords)\n",
    "    df[\"description\"] = df.description.apply(stemSentence)\n",
    "    \n",
    "    df[\"title\"] = df.title.apply(remove_URL)\n",
    "    df[\"title\"] = df.title.apply(remove_html)\n",
    "    df[\"title\"] = df.title.apply(remove_emoji)\n",
    "    df[\"title\"] = df.title.apply(remove_dirty_words)\n",
    "    df[\"title\"] = df.title.apply(remove_punctuation)\n",
    "    df[\"title\"] = df.title.apply(remove_stopwords)\n",
    "    df[\"title\"] = df.title.apply(stemSentence)\n",
    "    \n",
    "    df[\"benefits\"] = df.benefits.apply(remove_URL)\n",
    "    df[\"benefits\"] = df.benefits.apply(remove_html)\n",
    "    df[\"benefits\"] = df.benefits.apply(remove_emoji)\n",
    "    df[\"benefits\"] = df.benefits.apply(remove_dirty_words)\n",
    "    df[\"benefits\"] = df.benefits.apply(remove_punctuation)\n",
    "    df[\"benefits\"] = df.benefits.apply(remove_stopwords)\n",
    "    df[\"benefits\"] = df.benefits.apply(stemSentence)\n",
    "    \n",
    "    df[\"requirements\"] = df.requirements.apply(remove_URL)\n",
    "    df[\"requirements\"] = df.requirements.apply(remove_html)\n",
    "    df[\"requirements\"] = df.requirements.apply(remove_emoji)\n",
    "    df[\"requirements\"] = df.requirements.apply(remove_dirty_words)\n",
    "    df[\"requirements\"] = df.requirements.apply(remove_punctuation)\n",
    "    df[\"requirements\"] = df.requirements.apply(remove_stopwords)\n",
    "    df[\"requirements\"] = df.requirements.apply(stemSentence)\n",
    "    \n",
    "    \n",
    "    # Feature Extraction\n",
    "    \n",
    "    if condition == \"test\":\n",
    "        with open('./pickle/vec_description.pkl', 'rb') as f:\n",
    "            vec_description = pickle.load(f)\n",
    "        \n",
    "        with open('./pickle/vec_title.pkl', 'rb') as f:\n",
    "            vec_title = pickle.load(f)\n",
    "            \n",
    "        with open('./pickle/vec_benefits.pkl', 'rb') as f:\n",
    "            vec_benefits = pickle.load(f)\n",
    "            \n",
    "        with open('./pickle/vec_requirements.pkl', 'rb') as f:\n",
    "            vec_requirements = pickle.load(f)\n",
    "        \n",
    "        tfidf_description = vec_description.transform(df[\"description\"])\n",
    "        features_name_desc = vec_description.get_feature_names_out() + \"_desc\"\n",
    "        desc_features = pd.DataFrame(data = tfidf_description.toarray(), columns = features_name_desc)\n",
    "\n",
    "        tfidf_requirements = vec_requirements.transform(df[\"requirements\"])\n",
    "        features_name_req = vec_requirements.get_feature_names_out() + \"_req\"\n",
    "        req_features = pd.DataFrame(data = tfidf_requirements.toarray(), columns = features_name_req)\n",
    "\n",
    "        tfidf_title = vec_title.transform(df[\"title\"])\n",
    "        features_name_title = vec_title.get_feature_names_out() + \"_title\"\n",
    "        title_features = pd.DataFrame(data = tfidf_title.toarray(), columns = features_name_title)\n",
    "        \n",
    "        tfidf_benefits = vec_benefits.transform(df[\"benefits\"])\n",
    "        features_name_benefits = vec_benefits.get_feature_names_out() +\"_benefits\"\n",
    "        benefits_features = pd.DataFrame(data = tfidf_benefits.toarray(), columns = features_name_benefits)\n",
    "        \n",
    "        feature_text = pd.concat([desc_features, req_features, title_features, benefits_features], axis=1)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        vec_description = TfidfVectorizer(smooth_idf=True)\n",
    "        tfidf_description = vec_description.fit_transform(df[\"description\"])\n",
    "        features_name_desc = vec_description.get_feature_names_out() + \"_desc\"\n",
    "        desc_features = pd.DataFrame(data = tfidf_description.toarray(), columns = features_name_desc)\n",
    "\n",
    "        vec_requirements = TfidfVectorizer(smooth_idf=True)\n",
    "        tfidf_requirements = vec_requirements.fit_transform(df[\"requirements\"])\n",
    "        features_name_req = vec_requirements.get_feature_names_out() + \"_req\"\n",
    "        req_features = pd.DataFrame(data = tfidf_requirements.toarray(), columns = features_name_req)\n",
    "\n",
    "        vec_title = TfidfVectorizer(smooth_idf=True)\n",
    "        tfidf_title = vec_title.fit_transform(df[\"title\"])\n",
    "        features_name_title = vec_title.get_feature_names_out() + \"_title\"\n",
    "        title_features = pd.DataFrame(data = tfidf_title.toarray(), columns = features_name_title)\n",
    "\n",
    "        vec_benefits = TfidfVectorizer(smooth_idf=True)\n",
    "        tfidf_benefits = vec_benefits.fit_transform(df[\"benefits\"])\n",
    "        features_name_benefits = vec_benefits.get_feature_names_out() + \"_benefits\"\n",
    "        benefits_features = pd.DataFrame(data = tfidf_benefits.toarray(), columns = features_name_benefits)\n",
    "        \n",
    "        feature_text = pd.concat([desc_features, req_features, title_features, benefits_features], axis=1)\n",
    "        \n",
    "        with open('./pickle/vec_description.pkl', 'wb') as f:\n",
    "            pickle.dump(vec_description, f)\n",
    "        \n",
    "        with open('./pickle/vec_title.pkl', 'wb') as f:\n",
    "            pickle.dump(vec_title, f)\n",
    "            \n",
    "        with open('./pickle/vec_benefits.pkl', 'wb') as f:\n",
    "            pickle.dump(vec_benefits, f)\n",
    "            \n",
    "        with open('./pickle/vec_requirements.pkl', 'wb') as f:\n",
    "            pickle.dump(vec_requirements, f)\n",
    "        \n",
    "    return feature_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dae20e-8002-45fe-a2a6-7103a4b1d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_processing(df):\n",
    "    def extract_state(s):\n",
    "        \"\"\" Extract state from the location\"\"\"\n",
    "        \"\"\" The function can be used only when the state is formmated with two capital letter\"\"\"\n",
    "        \"\"\" Input: Series, iterable object\"\"\"\n",
    "        \"\"\" Output: List of States\"\"\"\n",
    "    \n",
    "        s.fillna(\"No Location\", inplace = True)\n",
    "        result = []    \n",
    "        for i in np.arange(len(s)):\n",
    "            if (s[i].__contains__(\"US\")):\n",
    "                extracted = re.findall(r'[A-Z]{2}', re.sub(r'[US]','',s[i]))\n",
    "                # Edge Case 1: Posting is from US but State is not posted\n",
    "                if extracted == []:\n",
    "                    extracted = [\"Domestic\"]\n",
    "                # Edge Case 2: Regex detect a city name as a State name\n",
    "                if len(extracted) != 1:\n",
    "                    while len(extracted) > 1:\n",
    "                        extracted.pop()\n",
    "                result += extracted\n",
    "            else:\n",
    "                # Edge Case 3: Location is not given \n",
    "                if s[i] == [\"No Location\"]:\n",
    "                    result += s[i]\n",
    "                # Edge Case 4: Location is given but not in US\n",
    "                elif re.findall(r'[A-Z]{2}', s[i]) != []:\n",
    "                    result += [\"Foreign\"]\n",
    "                # Edge Case 5: Location cannot be identified from the given information\n",
    "                else:\n",
    "                    result += [\"No Location\"]\n",
    "        return result\n",
    "\n",
    "    result = extract_state(df[\"location\"])\n",
    "    df[\"state\"] = result\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1296a9f4-9c66-4a3f-9ecf-b2508a7da36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE_processing(df, condition = \"train\"):\n",
    "    \"\"\"One Hot Encoding of all categorical variables\"\"\"\n",
    "    \"\"\"Input: Data Frame\"\"\"\n",
    "    \"\"\"Output: Data Frame of dummy variables\"\"\"\n",
    "    \n",
    "    if condition == \"test\":\n",
    "        with open('./pickle/encoder_func.pkl', 'rb') as f:\n",
    "            encoder_func = pickle.load(f)\n",
    "        \n",
    "        with open('./pickle/encoder_et.pkl', 'rb') as f:\n",
    "            encoder_et = pickle.load(f)\n",
    "            \n",
    "        with open('./pickle/encoder_re.pkl', 'rb') as f:\n",
    "            encoder_re = pickle.load(f)\n",
    "            \n",
    "        with open('./pickle/encoder_ind.pkl', 'rb') as f:\n",
    "            encoder_ind = pickle.load(f)\n",
    "        \n",
    "        with open('./pickle/encoder_state.pkl', 'rb') as f:\n",
    "            encoder_state = pickle.load(f)\n",
    "            \n",
    "        \n",
    "        df[\"function\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"employment_type\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"required_experience\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"industry\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"state\"].fillna(\"NAN\", inplace = True)\n",
    "\n",
    "        encode_function = encoder_func.transform(df[['function']])\n",
    "        feature_name_func = encoder_func.get_feature_names_out()\n",
    "        encoder_df_func = pd.DataFrame(encode_function.toarray(), columns = feature_name_func)\n",
    "\n",
    "        encode_et = encoder_et.transform(df[['employment_type']])\n",
    "        feature_name_et = encoder_et.get_feature_names_out()\n",
    "        encoder_df_et = pd.DataFrame(encode_et.toarray(), columns = feature_name_et)\n",
    "\n",
    "        encode_re = encoder_re.transform(df[['required_experience']])\n",
    "        feature_name_re = encoder_re.get_feature_names_out()\n",
    "        encoder_df_re = pd.DataFrame(encode_re.toarray(), columns = feature_name_re)\n",
    "\n",
    "        encode_ind = encoder_ind.transform(df[['industry']])\n",
    "        feature_name_ind = encoder_ind.get_feature_names_out()\n",
    "        encoder_df_ind = pd.DataFrame(encode_ind.toarray(), columns = feature_name_ind)\n",
    "        \n",
    "        encode_state = encoder_state.transform(df[['state']])\n",
    "        feature_name_state = encoder_state.get_feature_names_out()\n",
    "        encoder_df_state = pd.DataFrame(encode_state.toarray(), columns = feature_name_state)\n",
    "\n",
    "        ohe_feature = pd.concat([encoder_df_func, encoder_df_et, encoder_df_re, encoder_df_ind], axis=1)\n",
    "    \n",
    "    else:   \n",
    "        df[\"function\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"employment_type\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"required_experience\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"industry\"].fillna(\"NAN\", inplace = True)\n",
    "        df[\"state\"].fillna(\"NAN\", inplace = True)\n",
    "\n",
    "        encoder_state = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        encode_state = encoder_state.fit_transform(df[['state']])\n",
    "        feature_name_state = encoder_state.get_feature_names_out()\n",
    "        encoder_df_state = pd.DataFrame(encode_state.toarray(), columns = feature_name_state)\n",
    "        \n",
    "        encoder_func = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        encode_function = encoder_func.fit_transform(df[['function']])\n",
    "        feature_name_func = encoder_func.get_feature_names_out()\n",
    "        encoder_df_func = pd.DataFrame(encode_function.toarray(), columns = feature_name_func)\n",
    "\n",
    "        encoder_et = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        encode_et = encoder_et.fit_transform(df[['employment_type']])\n",
    "        feature_name_et = encoder_et.get_feature_names_out()\n",
    "        encoder_df_et = pd.DataFrame(encode_et.toarray(), columns = feature_name_et)\n",
    "\n",
    "        encoder_re = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        encode_re = encoder_re.fit_transform(df[['required_experience']])\n",
    "        feature_name_re = encoder_re.get_feature_names_out()\n",
    "        encoder_df_re = pd.DataFrame(encode_re.toarray(), columns = feature_name_re)\n",
    "\n",
    "        encoder_ind = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        encode_ind = encoder_ind.fit_transform(df[['industry']])\n",
    "        feature_name_ind = encoder_ind.get_feature_names_out()\n",
    "        encoder_df_ind = pd.DataFrame(encode_ind.toarray(), columns = feature_name_ind)\n",
    "\n",
    "        ohe_feature = pd.concat([encoder_df_func, encoder_df_et, encoder_df_re, encoder_df_ind], axis=1)\n",
    "        \n",
    "        with open('./pickle/encoder_func.pkl', 'wb') as f:\n",
    "                pickle.dump(encoder_func, f)\n",
    "\n",
    "        with open('./pickle/encoder_et.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder_et, f)\n",
    "\n",
    "        with open('./pickle/encoder_re.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder_re, f)\n",
    "\n",
    "        with open('./pickle/encoder_ind.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder_ind, f)\n",
    "            \n",
    "        with open('./pickle/encoder_state.pkl', 'wb') as f:\n",
    "            pickle.dump(encoder_state, f)\n",
    "    \n",
    "    return ohe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd92af2f-19a5-4d7f-abd9-a8744161ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_processing(df):\n",
    "    \"\"\" Delete Unused columns, binarization and location extraction\"\"\"\n",
    "    \"\"\" Input: Data Frame \"\"\"\n",
    "    \"\"\" Output: processed Data Frame\"\"\"\n",
    "        \n",
    "    # Remove job_id, department, salary_range and location\n",
    "    \n",
    "    df = df.drop(['job_id', 'department', 'salary_range', 'location'], axis=1)\n",
    "    df = df.drop(['description', 'requirements', 'title', 'benefits'], axis=1)\n",
    "    df = df.drop(['function', 'employment_type', 'required_experience', 'industry', 'state'], axis=1)\n",
    "    \n",
    "    # binarize company_profile and required_education \n",
    "    \n",
    "    bool_series_cp = pd.isnull(df[\"company_profile\"])\n",
    "    df[\"company_profile\"][bool_series_cp] = 1\n",
    "    df[\"company_profile\"][~bool_series_cp] = 0 \n",
    "    \n",
    "    bool_series_re = pd.isnull(df[\"required_education\"])\n",
    "    df[\"required_education\"][bool_series_re] = 1\n",
    "    df[\"required_education\"][~bool_series_re] = 0 \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f5c5-c2df-4282-b841-554c7beaa476",
   "metadata": {},
   "source": [
    "Please follow the following steps to process your data. Otherwise, you will face MemoryError. \n",
    "\n",
    "```python\n",
    "train_data = pd.read_csv(\"./data/train_set.csv\") #Load Data \n",
    "text_features_train = text_processing(train_data) # Text Processing (Take Long Time)\n",
    "joblib.dump(text_features_train, './data/text_features_train_jlib') # Save it as jlib file \n",
    "train_data = location_processing(train_data) # add State to train data \n",
    "OHE_features_train = OHE_processing(train_data) # OHE\n",
    "joblib.dump(OHE_features_train, './data/OHE_features_train_jlib') # Save OHE features as jlib file \n",
    "processed_train = final_processing(train_data) # Final Processing (will get several warning, ignore)\n",
    "joblib.dump(processed_train, './data/processed_train_jlib') # Save processed train_data as jlib file\n",
    "```\n",
    "Combine these three files to get a whole matrix. \n",
    "\n",
    "Here is how you unpack the joblib file:\n",
    "\n",
    "```python\n",
    "text_features_train = joblib.load('./data/text_features_train_jlib')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bee93da-dd77-4c07-9563-a0a1a7e17672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>...</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7530</td>\n",
       "      <td>7531</td>\n",
       "      <td>contact center repres</td>\n",
       "      <td>US, VA, Virginia Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidewater Finance Co. was established in 1992 ...</td>\n",
       "      <td>tidewat financ compani locat virginia beach va...</td>\n",
       "      <td>posit requir follow qualif minimum year call c...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "      <td>VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>130</td>\n",
       "      <td>custom servic associ</td>\n",
       "      <td>US, TX, Dallas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Novitex Enterprise Solutions, formerly Pitney ...</td>\n",
       "      <td>custom servic associ base dalla tx right candi...</td>\n",
       "      <td>qualificationsminimum year custom servic relat...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>High School or equivalent</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4640</td>\n",
       "      <td>4641</td>\n",
       "      <td>autom test analyst</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SilverStripe CMS &amp;amp; Framework is an open so...</td>\n",
       "      <td>look dedic passion softwar test analyst team p...</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0</td>\n",
       "      <td>Foreign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>402</td>\n",
       "      <td>403</td>\n",
       "      <td>insid sale profession omaha</td>\n",
       "      <td>US, NE, Omaha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABC Supply Co., Inc. is the nation’s largest w...</td>\n",
       "      <td>sale repr provid assist custom purcha materi t...</td>\n",
       "      <td>sale repres must abil provid superior custom s...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Building Materials</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13218</td>\n",
       "      <td>13219</td>\n",
       "      <td>content market seo manag</td>\n",
       "      <td>US, CA, Los Angeles</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MeUndies is a lifestyle brand that is transfor...</td>\n",
       "      <td>meundi lifestyl brand transform way peopl perc...</td>\n",
       "      <td>requir qualif person attributesy year directli...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>14299</td>\n",
       "      <td>1841</td>\n",
       "      <td>1842</td>\n",
       "      <td>public relat manag</td>\n",
       "      <td>US, CA, Irvine</td>\n",
       "      <td>PR</td>\n",
       "      <td>60000-80000</td>\n",
       "      <td>HappyFox is a young startup that is all about ...</td>\n",
       "      <td>happyfox young web saa startup bring happi quo...</td>\n",
       "      <td>year experi relev pr proven track record take ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>Public Relations</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14300</th>\n",
       "      <td>14300</td>\n",
       "      <td>11852</td>\n",
       "      <td>11853</td>\n",
       "      <td>io develop</td>\n",
       "      <td>GR, I, Athens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tizU is a free iphone app that allows you to s...</td>\n",
       "      <td>tizu mobil app allow send hidden messag friend...</td>\n",
       "      <td>year io developmenthav publish least one origi...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0</td>\n",
       "      <td>Foreign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>14301</td>\n",
       "      <td>10870</td>\n",
       "      <td>10871</td>\n",
       "      <td>databas research</td>\n",
       "      <td>US, NJ, Elmwood Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000-20000</td>\n",
       "      <td>With over 1,300 investment professionals locat...</td>\n",
       "      <td>top real estat invest broker seek full time da...</td>\n",
       "      <td>requir skill excel microsoft offic comfort wit...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>High School or equivalent</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14302</th>\n",
       "      <td>14302</td>\n",
       "      <td>565</td>\n",
       "      <td>566</td>\n",
       "      <td>custom servic associ</td>\n",
       "      <td>US, AZ, Phoenix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Novitex Enterprise Solutions, formerly Pitney ...</td>\n",
       "      <td>custom servic associ base phoenix az right can...</td>\n",
       "      <td>minimum requir minimum month custom servic hos...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>High School or equivalent</td>\n",
       "      <td>Legal Services</td>\n",
       "      <td>Administrative</td>\n",
       "      <td>0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>14303</td>\n",
       "      <td>13398</td>\n",
       "      <td>13399</td>\n",
       "      <td>super market specialist</td>\n",
       "      <td>CA, ON, Toronto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>want grow dad ask marin biologist astronaut do...</td>\n",
       "      <td>killer market instinct know sell understand hu...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>Foreign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14304 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  job_id                         title  \\\n",
       "0                 0        7530    7531        contact center repres    \n",
       "1                 1         129     130         custom servic associ    \n",
       "2                 2        4640    4641           autom test analyst    \n",
       "3                 3         402     403  insid sale profession omaha    \n",
       "4                 4       13218   13219     content market seo manag    \n",
       "...             ...         ...     ...                           ...   \n",
       "14299         14299        1841    1842           public relat manag    \n",
       "14300         14300       11852   11853                   io develop    \n",
       "14301         14301       10870   10871             databas research    \n",
       "14302         14302         565     566         custom servic associ    \n",
       "14303         14303       13398   13399      super market specialist    \n",
       "\n",
       "                     location department salary_range  \\\n",
       "0      US, VA, Virginia Beach        NaN          NaN   \n",
       "1              US, TX, Dallas        NaN          NaN   \n",
       "2             NZ, , Auckland   Permanent          NaN   \n",
       "3               US, NE, Omaha        NaN          NaN   \n",
       "4         US, CA, Los Angeles  Marketing          NaN   \n",
       "...                       ...        ...          ...   \n",
       "14299          US, CA, Irvine         PR  60000-80000   \n",
       "14300           GR, I, Athens        NaN          NaN   \n",
       "14301    US, NJ, Elmwood Park        NaN  20000-20000   \n",
       "14302         US, AZ, Phoenix        NaN          NaN   \n",
       "14303         CA, ON, Toronto        NaN          NaN   \n",
       "\n",
       "                                         company_profile  \\\n",
       "0      Tidewater Finance Co. was established in 1992 ...   \n",
       "1      Novitex Enterprise Solutions, formerly Pitney ...   \n",
       "2      SilverStripe CMS &amp; Framework is an open so...   \n",
       "3      ABC Supply Co., Inc. is the nation’s largest w...   \n",
       "4      MeUndies is a lifestyle brand that is transfor...   \n",
       "...                                                  ...   \n",
       "14299  HappyFox is a young startup that is all about ...   \n",
       "14300  tizU is a free iphone app that allows you to s...   \n",
       "14301  With over 1,300 investment professionals locat...   \n",
       "14302  Novitex Enterprise Solutions, formerly Pitney ...   \n",
       "14303                                                NaN   \n",
       "\n",
       "                                             description  \\\n",
       "0      tidewat financ compani locat virginia beach va...   \n",
       "1      custom servic associ base dalla tx right candi...   \n",
       "2      look dedic passion softwar test analyst team p...   \n",
       "3      sale repr provid assist custom purcha materi t...   \n",
       "4      meundi lifestyl brand transform way peopl perc...   \n",
       "...                                                  ...   \n",
       "14299  happyfox young web saa startup bring happi quo...   \n",
       "14300  tizu mobil app allow send hidden messag friend...   \n",
       "14301  top real estat invest broker seek full time da...   \n",
       "14302  custom servic associ base phoenix az right can...   \n",
       "14303  want grow dad ask marin biologist astronaut do...   \n",
       "\n",
       "                                            requirements  ... telecommuting  \\\n",
       "0      posit requir follow qualif minimum year call c...  ...             0   \n",
       "1      qualificationsminimum year custom servic relat...  ...             0   \n",
       "2                                                         ...             0   \n",
       "3      sale repres must abil provid superior custom s...  ...             0   \n",
       "4      requir qualif person attributesy year directli...  ...             0   \n",
       "...                                                  ...  ...           ...   \n",
       "14299  year experi relev pr proven track record take ...  ...             0   \n",
       "14300  year io developmenthav publish least one origi...  ...             0   \n",
       "14301  requir skill excel microsoft offic comfort wit...  ...             0   \n",
       "14302  minimum requir minimum month custom servic hos...  ...             0   \n",
       "14303  killer market instinct know sell understand hu...  ...             0   \n",
       "\n",
       "       has_company_logo  has_questions  employment_type required_experience  \\\n",
       "0                     1              0        Full-time         Entry level   \n",
       "1                     1              0        Full-time         Entry level   \n",
       "2                     1              1        Full-time    Mid-Senior level   \n",
       "3                     1              0        Full-time                 NAN   \n",
       "4                     1              0        Full-time    Mid-Senior level   \n",
       "...                 ...            ...              ...                 ...   \n",
       "14299                 1              1         Contract           Associate   \n",
       "14300                 1              1              NAN                 NAN   \n",
       "14301                 1              0        Full-time         Entry level   \n",
       "14302                 1              0        Full-time         Entry level   \n",
       "14303                 0              0        Full-time                 NAN   \n",
       "\n",
       "              required_education                             industry  \\\n",
       "0                    Unspecified                   Financial Services   \n",
       "1      High School or equivalent                   Telecommunications   \n",
       "2                            NaN  Information Technology and Services   \n",
       "3                            NaN                   Building Materials   \n",
       "4              Bachelor's Degree                             Internet   \n",
       "...                          ...                                  ...   \n",
       "14299          Bachelor's Degree  Information Technology and Services   \n",
       "14300                        NaN                                  NAN   \n",
       "14301  High School or equivalent                          Real Estate   \n",
       "14302  High School or equivalent                       Legal Services   \n",
       "14303                        NaN  Information Technology and Services   \n",
       "\n",
       "               function fraudulent    state  \n",
       "0      Customer Service          0       VA  \n",
       "1      Customer Service          0       TX  \n",
       "2                   NAN          0  Foreign  \n",
       "3                 Sales          0       NE  \n",
       "4             Marketing          0       CA  \n",
       "...                 ...        ...      ...  \n",
       "14299  Public Relations          0       CA  \n",
       "14300               NAN          0  Foreign  \n",
       "14301               NAN          0       NJ  \n",
       "14302    Administrative          0       AZ  \n",
       "14303         Marketing          0  Foreign  \n",
       "\n",
       "[14304 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b588579-d941-45c3-93e0-bd27499c134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = location_processing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba0a7155-81b5-40b6-9e70-acd0392f2c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE_features_train = OHE_processing(train_data.iloc[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2deb3663-5ff9-4ad2-a33d-6588b8a22312",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ef6b2f-07f0-45bc-84b6-956cbfd821b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_train = text_processing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c961b0-6923-46e1-b531-edd8dcff47e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_desc</th>\n",
       "      <th>aaa_desc</th>\n",
       "      <th>aaab_desc</th>\n",
       "      <th>aab_desc</th>\n",
       "      <th>aabc_desc</th>\n",
       "      <th>aabd_desc</th>\n",
       "      <th>aabf_desc</th>\n",
       "      <th>aac_desc</th>\n",
       "      <th>aaccd_desc</th>\n",
       "      <th>aachen_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>zodat_benefits</th>\n",
       "      <th>zollman_benefits</th>\n",
       "      <th>zombi_benefits</th>\n",
       "      <th>zone_benefits</th>\n",
       "      <th>zoo_benefits</th>\n",
       "      <th>zowel_benefits</th>\n",
       "      <th>zu_benefits</th>\n",
       "      <th>zult_benefits</th>\n",
       "      <th>zutrifft_benefits</th>\n",
       "      <th>zweig_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14300</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14302</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14304 rows × 89527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa_desc  aaa_desc  aaab_desc  aab_desc  aabc_desc  aabd_desc  \\\n",
       "0      0.165596       0.0        0.0       0.0        0.0        0.0   \n",
       "1      0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "2      0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "3      0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "4      0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "...         ...       ...        ...       ...        ...        ...   \n",
       "14299  0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "14300  0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "14301  0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "14302  0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "14303  0.000000       0.0        0.0       0.0        0.0        0.0   \n",
       "\n",
       "       aabf_desc  aac_desc  aaccd_desc  aachen_desc  ...  zodat_benefits  \\\n",
       "0            0.0       0.0         0.0          0.0  ...             0.0   \n",
       "1            0.0       0.0         0.0          0.0  ...             0.0   \n",
       "2            0.0       0.0         0.0          0.0  ...             0.0   \n",
       "3            0.0       0.0         0.0          0.0  ...             0.0   \n",
       "4            0.0       0.0         0.0          0.0  ...             0.0   \n",
       "...          ...       ...         ...          ...  ...             ...   \n",
       "14299        0.0       0.0         0.0          0.0  ...             0.0   \n",
       "14300        0.0       0.0         0.0          0.0  ...             0.0   \n",
       "14301        0.0       0.0         0.0          0.0  ...             0.0   \n",
       "14302        0.0       0.0         0.0          0.0  ...             0.0   \n",
       "14303        0.0       0.0         0.0          0.0  ...             0.0   \n",
       "\n",
       "       zollman_benefits  zombi_benefits  zone_benefits  zoo_benefits  \\\n",
       "0                   0.0             0.0            0.0           0.0   \n",
       "1                   0.0             0.0            0.0           0.0   \n",
       "2                   0.0             0.0            0.0           0.0   \n",
       "3                   0.0             0.0            0.0           0.0   \n",
       "4                   0.0             0.0            0.0           0.0   \n",
       "...                 ...             ...            ...           ...   \n",
       "14299               0.0             0.0            0.0           0.0   \n",
       "14300               0.0             0.0            0.0           0.0   \n",
       "14301               0.0             0.0            0.0           0.0   \n",
       "14302               0.0             0.0            0.0           0.0   \n",
       "14303               0.0             0.0            0.0           0.0   \n",
       "\n",
       "       zowel_benefits  zu_benefits  zult_benefits  zutrifft_benefits  \\\n",
       "0                 0.0          0.0            0.0                0.0   \n",
       "1                 0.0          0.0            0.0                0.0   \n",
       "2                 0.0          0.0            0.0                0.0   \n",
       "3                 0.0          0.0            0.0                0.0   \n",
       "4                 0.0          0.0            0.0                0.0   \n",
       "...               ...          ...            ...                ...   \n",
       "14299             0.0          0.0            0.0                0.0   \n",
       "14300             0.0          0.0            0.0                0.0   \n",
       "14301             0.0          0.0            0.0                0.0   \n",
       "14302             0.0          0.0            0.0                0.0   \n",
       "14303             0.0          0.0            0.0                0.0   \n",
       "\n",
       "       zweig_benefits  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "14299             0.0  \n",
       "14300             0.0  \n",
       "14301             0.0  \n",
       "14302             0.0  \n",
       "14303             0.0  \n",
       "\n",
       "[14304 rows x 89527 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cc43b-8767-4282-a013-c0a12cbc9af8",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Saving these file in csv or pickle can cause memory error because of its large file size. \n",
    "To store array-like object, joblib works better than pickle. Pickle is useful when we need to store non-array object like encoders and models.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
