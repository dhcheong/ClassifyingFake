{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183f6f0b-6e01-42f2-9878-4c6e0bae76d2",
   "metadata": {},
   "source": [
    "# Creating Features from the Description\n",
    "\n",
    "As we mentioned earlier, we want each word to be a feature of the dataset. To do this, we will use the **Bag of Words** method that will ensure each description in the dataset is represented only with the word count (e.g., if the description contains the word, the column will mark it as 1, otherwise 0). However, only using the Bag of Words is somewhat problematic. Since frequency is dependent on the length of the text, longer texts can calculate a higher frequency for some unnecessary words that rarely appear across the data as a whole. This clearly can cause big trouble when we come to feature selection. \n",
    "\n",
    "To remedy this problem, we will standardize the word frequency using **TF-IDF (Term Frequency-Inverse Document Frequency)** so that all frequencies can be weighted. It is a numerical statistic intended to reflect how important a word is to a document in a collection, and here is how we calculate it. \n",
    "\n",
    "$$ \\text{TF-IDF} = \\text{TF} \\times \\text{IDF} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$TF = \\frac{\\text{number of times the word appears in the description}}{\\text{total number of the word in the entire dataset}}$$\n",
    "\n",
    "$$IDF = \\log ( \\frac{\\text{number of description in the dataset}}{\\text{number of description that contained the word}} )$$\n",
    "\n",
    "Term Frequency (TF) measures how frequently a term occurs in a document. Inverse Document Frequency (IDF) is a factor that diminishes the weight of terms that occur very frequently in the document and increases the weight of words that occur rarely. As you observe here, as the word appears less frequently throughout the dataset, the IDF increases which decreases TF-IDF as a result. We gives more weight on the words that appear frequently across the entire dataset. This way, we can avoid possible outlier/confounding features in our dataset.\n",
    "\n",
    "```{note}\n",
    "In the original project, we had to extracting the features and converting those using TF-IDF were a seperate step. In Python, sklearn has very useful feature extraction encoder, \"TfidfVectorizer\", and this combines the extraction and conversion into a single step.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4902888-b799-4c6a-a4c5-d89415ae33fa",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad43e9fe-fca0-4230-972b-b881cd915a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train_set.csv\")\n",
    "train_data[\"description\"].fillna('no_description', inplace = True)\n",
    "vec = TfidfVectorizer(smooth_idf=True)\n",
    "tfidf_description = vec.fit_transform(train_data[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca39660-3411-40c2-98e6-e3122d7232c2",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "We have some NAs in the description column, which will cause the error with TfidfVectorizer if we don't impute it. I decided to replace all NAs with \"no_description\" because NA may indicate a strong signal for fraudulent. For instance, a fraudulent posting might have no job description.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905d8634-f5af-4897-86d3-7cd37460d0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaab</th>\n",
       "      <th>aab</th>\n",
       "      <th>aabc</th>\n",
       "      <th>aabd</th>\n",
       "      <th>aabf</th>\n",
       "      <th>aac</th>\n",
       "      <th>aaccd</th>\n",
       "      <th>aachen</th>\n",
       "      <th>...</th>\n",
       "      <th>zumero</th>\n",
       "      <th>zur</th>\n",
       "      <th>zurb</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zusammenarbeitest</th>\n",
       "      <th>zusammenbringt</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zyfax</th>\n",
       "      <th>zyka</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa  aaa  aaab  aab  aabc  aabd  aabf  aac  aaccd  aachen  ...  \\\n",
       "0  0.164839  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "1  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "2  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "3  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "4  0.000000  0.0   0.0  0.0   0.0   0.0   0.0  0.0    0.0     0.0  ...   \n",
       "\n",
       "   zumero  zur  zurb  zurich  zusammenarbeitest  zusammenbringt  zweig  zyfax  \\\n",
       "0     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "1     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "2     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "3     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "4     0.0  0.0   0.0     0.0                0.0             0.0    0.0    0.0   \n",
       "\n",
       "   zyka  zynga  \n",
       "0   0.0    0.0  \n",
       "1   0.0    0.0  \n",
       "2   0.0    0.0  \n",
       "3   0.0    0.0  \n",
       "4   0.0    0.0  \n",
       "\n",
       "[5 rows x 40989 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name = vec.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_description.toarray(), columns = features_name)\n",
    "df_tfidfvect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf9395-b9dd-4f21-8016-bfd153f7a4ed",
   "metadata": {},
   "source": [
    "We will save our TfidfVectorizer as a pickle so that we can use it later to the test dataset.\n",
    "\n",
    "```python\n",
    "with open('./pickle/tfidfvec.pkl', 'wb') as f:\n",
    "    pickle.dump(vec, f)\n",
    "```\n",
    "```{warning}\n",
    "This step is essential because if we don't use the same encoder to train and test the set, it might raise the dimension error in any machine learning algorithm. For example, if we use a different encoder, the feature \"planner\" in the training set will not appear in the test set so that we can get the numbers of a feature. To avoid this error, we should apply the encoder that was trained by the training set to the test set. Sometimes, people combine train and test sets and run TfidfVectorizer to avoid this error, but we must always assume that we don't have any test set yet.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
